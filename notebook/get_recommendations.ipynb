{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Unstructured with Structured Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necesssary libraries.\n",
    "\n",
    "### 1. SETUP\n",
    "To prepare your environment, you need to install some packages and enter credentials for the Watson services.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watson-developer-cloud==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mammoth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install textract\n",
    "Please Follow the instructions to install textract on your system\n",
    "https://textract.readthedocs.io/en/v1.2.0/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import packages and libraries\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import os, sys, glob, mammoth\n",
    "\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 \\\n",
    "  import Features, EntitiesOptions, SemanticRolesOptions, RelationsOptions, KeywordsOptions\n",
    "\n",
    "import PyPDF2 \n",
    "import textract\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Fill the path to configuration.json file \n",
    "'''\n",
    "#input_file  = open(\"< path to /configuration.json>\", \"rb\")\n",
    "config_classification_json = json.loads(input_file.read())\n",
    "#print(config_classification_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the structured data from Data.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stu_df = pd.read_csv(<\"path to/Data.csv\">)\n",
    "stu_df\n",
    "# stu_df_columns_list = list(stu_df.columns.values)\n",
    "# stu_df_columns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add your service credentials from Bluemix for the Watson services.\n",
    "You must create a Watson Natural Language Understanding service on Bluemix. Create a service for Natural Language Understanding (NLU). Insert the username and password values for your NLU in the following cell. Do not change the values of the version fields.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "   username= \"\",\n",
    "  password= \"\",\n",
    "  version='2017-02-27')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Functions for  Watson Text Classification\n",
    "Write the classification related utility functions in a modularalized form with augmentation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_using_NLU(text_content):\n",
    "    '''\n",
    "    Call Watson Natural Language Understanding service to obtain analysis results.\n",
    "    '''\n",
    "    response = natural_language_understanding.analyze(\n",
    "        text= text_content,\n",
    "        features=Features(\n",
    "        entities=EntitiesOptions(),\n",
    "        relations=RelationsOptions(),\n",
    "        keywords= KeywordsOptions())\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_NLUResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    \n",
    "\n",
    "def classify_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = analyze_using_NLU(text)\n",
    "    responsejson = response\n",
    "    \n",
    "    sentenceList = split_sentences(text)\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = config\n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        for steps in stages['steps']:\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            augument_NLUResponse(responsejson,'entities',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                augument_NLUResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            augument_NLUResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "def replace_unicode_strings(response):\n",
    "    \"\"\" Convert dict with unicode strings to strings.\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n",
    "    elif isinstance(response, list):\n",
    "        return [replace_unicode_strings(element) for element in response]\n",
    "    elif isinstance(response, unicode):\n",
    "        return response.encode('utf-8')\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Requirements From the Job Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_1 = \"I need a candidate with User Experience Design skills and experience should be more than 24 months. \"\n",
    "job_description_2 = \"I need a candidate with Machine Learning Expert and experience should be more than 27 months.\"\n",
    "job_description = [job_description_1, job_description_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getrequirements(job_description):\n",
    "        requirement_jd=[]\n",
    "        for i in job_description:\n",
    "            requirement_jd.append(classify_text(i,config_classification_json))\n",
    "        return requirement_jd\n",
    "\n",
    "def getRequiredCandidateEntityList(requirement_jd):\n",
    "    RequiredCandidateEntityList =[]\n",
    "    entity_dict={}\n",
    "    text_type=''\n",
    "    text_value = ''\n",
    "    for i in requirement_jd:\n",
    "    entity_dict={}\n",
    "    for k in i['entities']:\n",
    "        for key1, value1 in k.items():\n",
    "            if(key1=='type'):\n",
    "                text_type = value1\n",
    "            if(key1=='text'):\n",
    "                text_value = value1\n",
    "        entity_dict[text_type] = text_value\n",
    "    RequiredCandidateEntityList.append(entity_dict)\n",
    "    return RequiredCandidateEntityList\n",
    "\n",
    "def getskills_matching_candidates(RequiredCandidateEntityList):\n",
    "    '''\n",
    "    Filtering the Candidates matching with the required skills.\n",
    "    '''\n",
    "    skills_matching_candidates =[]\n",
    "    row_list = []\n",
    "    for i in RequiredCandidateEntityList:\n",
    "        requirement_1= i['NAME'].lstrip()\n",
    "        requirement_2= i['Quantity'].lstrip()\n",
    "        for index, row in stu_df.iterrows():\n",
    "            if '/' or ',' in row['Skills']:\n",
    "                if requirement_1 in list(re.split('\\/|,',row['Skills'])):\n",
    "                    row_list.append(row)\n",
    "                    skills_matching_candidates.append(row['Name'])\n",
    "            else:\n",
    "                if(requirement_1 in row['Skills']):\n",
    "                    row_list.append(row)\n",
    "                    skills_matching_candidates.append(row['Name'])\n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement_jd = getrequirements(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RequiredCandidateEntityList = getRequiredCandidateEntityList(requirement_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = getskills_matching_candidates(RequiredCandidateEntityList)\n",
    "filtered_dataframe = pd.DataFrame(row_list)\n",
    "filtered_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Processing the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the path to CVs & Dataframe folder and uncomment this line.\n",
    "'''path = \"<path to /CVs & Dataframe/>\"\n",
    "'''\n",
    "\n",
    "def extractingTextfromresumes():\n",
    "    '''Extracting Text from the pool of resumes(processing word docs and pdfs)\n",
    "    '''\n",
    "    os.walk('src')\n",
    "    matching_candidates_text = []\n",
    "    filenames = glob.glob(path+'/*.pdf')\n",
    "    filenames_docx= glob.glob(path+'/*.docx')\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        pdfFileObj = open(filename,'rb')\n",
    "        #The pdfReader variable is a readable object that will be parsed\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        #discerning the number of pages will allow us to parse through all #the pages\n",
    "        num_pages = pdfReader.numPages\n",
    "        count = 0\n",
    "        text = \"\"\n",
    "        #The while loop will read each page\n",
    "        while count < num_pages:\n",
    "            pageObj = pdfReader.getPage(count)\n",
    "            count +=1\n",
    "            text += pageObj.extractText()\n",
    "        #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "            if text != \"\":\n",
    "                text = text\n",
    "        #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "            else:\n",
    "                text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "        # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "        # Now, we will clean our text variable, and return it as a list of keywords.\n",
    "        matching_candidates_text.append(text)\n",
    "\n",
    "    for filename in filenames_docx:\n",
    "        print(filename)\n",
    "        with open(filename, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value # The raw text\n",
    "            messages = result.messages # Any messages\n",
    "            matching_candidates_text.append(text)\n",
    "            \n",
    "    return matching_candidates_text\n",
    "\n",
    "def processTheTextWithWatsonNLU(matching_candidates_text):\n",
    "    '''\n",
    "    Process the text with Watson NLU\n",
    "    '''\n",
    "    NLU_Results_Matched_Candidates = []\n",
    "    for text in matching_candidates_text:\n",
    "        json = classify_text(text,config_classification_json)\n",
    "        NLU_Results_Matched_Candidates.append(json)\n",
    "    return NLU_Results_Matched_Candidates\n",
    "\n",
    "\n",
    "def unstructuredTexttoadataframe(NLU_Results_Matched_Candidates):\n",
    "    '''\n",
    "    Convert the unstructured text(entities in the result of NLU) to a dataframe\n",
    "    '''\n",
    "    matchedCandidateEntityList =[]\n",
    "    entity_dict={}\n",
    "    text_type=''\n",
    "    text_value = ''\n",
    "    for i in NLU_Results_Matched_Candidates:\n",
    "        entity_dict={}\n",
    "        for k in i['entities']:\n",
    "            for key1, value1 in k.items():\n",
    "                if(key1=='type'):\n",
    "                    text_type = value1\n",
    "                if(key1=='text'):\n",
    "                    text_value = value1\n",
    "            entity_dict[text_type] = text_value\n",
    "        matchedCandidateEntityList.append(entity_dict)\n",
    "    return matchedCandidateEntityList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_candidates_text = extractingTextfromresumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLU_Results_Matched_Candidates = processTheTextWithWatsonNLU(matching_candidates_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchedCandidateEntityList = unstructuredTexttoadataframe(NLU_Results_Matched_Candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df = pd.DataFrame(matchedCandidateEntityList)\n",
    "resume_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recommendation(resume_df):\n",
    "    recommendation=[]\n",
    "    display(HTML('<!DOCTYPE html><html><title>W3.CSS</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\"><h2>Recommendation</h2></html>'))\n",
    "    for index, row in resume_df.iterrows():\n",
    "        if (int(row['PhoneNumber']) in list(filtered_dataframe['Handphone'])):\n",
    "            applied_before = filtered_dataframe[filtered_dataframe['Handphone'] == int(row['PhoneNumber'])]['Applied Before'].iloc[0]\n",
    "            comments = filtered_dataframe[filtered_dataframe['Handphone'] == int(row['PhoneNumber'])]['Comments'].iloc[0]\n",
    "\n",
    "            name = filtered_dataframe[filtered_dataframe['Handphone'] == int(row['PhoneNumber'])]['Name'].iloc[0]\n",
    "\n",
    "            if(applied_before.lower() == 'yes'):\n",
    "                    print_card = \"Candidate \"+ name +\" \"+comments\n",
    "                    display(HTML('<!DOCTYPE html><html><title>W3.CSS</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\"><body><div class=\"w3-container\"><div class=\"w3-panel w3-card w3-red\"><p>'+ print_card +'</p></div></div></body></html>'))\n",
    "            else:\n",
    "                experience = filtered_dataframe[filtered_dataframe['Name'] == name]['Experience in Months'].iloc[0]             \n",
    "                line = requirement_2\n",
    "                matchObj = re.match( r'\\d{2}', line)\n",
    "                if(matchObj):\n",
    "                    if(int(matchObj.group()) <= experience):\n",
    "                        print_card = \"Candidate \" + name + \" matches both requirements\"\n",
    "                        display(HTML('<!DOCTYPE html><html><title>W3.CSS</title><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\"><body><div class=\"w3-container\"><div class=\"w3-panel w3-card w3-green\"><p>'+ print_card +'</p></div></div></body></html>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recommendation(resume_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
